{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d8fd750",
   "metadata": {},
   "source": [
    "## notebook for exploring similar terms/sentences\n",
    "\n",
    "Turns tweets/posts into 512d vectors using a pretrained model, after which we use dimensionality reduction algorithms to turn the 512d vectors into 2d. We can then use the 2d vectors to visualise these tweets/posts in an interactive graph together with an analyst (currently using the `bulk` package). It will allow us to highlight snippets that have a particular word in them, and see which other snippets are close by. \n",
    "\n",
    "This would help analysts explore similar text snippets, and \n",
    "\n",
    "1: Give them a better idea of the size and scope of the topics that they are interested in (denoted by those words)\n",
    "\n",
    "2: Provide inspiration for other words that could have something to do with that cluster, which can be used to bootstrap the SFLM model, or a spaCy model using `patterns` \n",
    "\n",
    "- [x] Load data\n",
    "- [x] load spacy arabic model\n",
    "    - Used distiluse-base-multilingual-cased-v1 instead of spacy\n",
    "- [x] Add spacy model to sklearn pipeline\n",
    "    - Used huggingface through embetter to get BERT model\n",
    "- [x] Prep and export dataset to show similar sentences through bulk\n",
    "    - [x] run text through embedding\n",
    "    - [x] UMAP to dim reduction\n",
    "    - [x] run bulk to create a small 2d graph of similar sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54f3cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9b5650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import tentaclio\n",
    "import embetter\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from embetter.grab import ColumnGrabber\n",
    "from embetter.text import SentenceEncoder\n",
    "\n",
    "import umap\n",
    "import hdbscan\n",
    "import sklearn.cluster as cluster\n",
    "from sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score\n",
    "\n",
    "from phoenix.common import artifacts, run_params, utils\n",
    "from phoenix.tag.labelling import prodigy_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f849801b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install embetter\n",
    "# !pip install \"embetter[sentence-tfm]\"\n",
    "# !pip install umap-learn hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3deea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.setup_notebook_output()\n",
    "utils.setup_notebook_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345f0ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prodigy_dmaps_df_path = f\"{artifacts.urls.get_local()}/prodigy/\"\n",
    "tweets_dmaps_path = f\"{artifacts.urls.get_local()}/prodigy/dmaps_jordan_tweets.csv\"\n",
    "written_path = \"/Users/andrewsutjahjo/git/python/phoenix/local_artifacts//prodigy/dmaps_jordan_tweets-11.csv\"\n",
    "\n",
    "output_path = f\"{artifacts.urls.get_local()}/prodigy/dmaps_jordan_tweets-11.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edfd0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(written_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be65369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1d2f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_emb_pipeline = make_pipeline(\n",
    "    ColumnGrabber(\"text\"),\n",
    "    SentenceEncoder(\"distiluse-base-multilingual-cased-v1\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcbfdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_array = text_emb_pipeline.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2771f235",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_embeddings = umap.UMAP().fit_transform(embeddings_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77876f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4660eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_embeddings.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef1e5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"x\"] = umap_embeddings[:,0]\n",
    "df[\"y\"] = umap_embeddings[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254fa968",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tentaclio.open(output_path, \"w\") as fb:\n",
    "    df.to_csv(fb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
