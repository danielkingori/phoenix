{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c89ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed078696",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70060758",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "    !nvidia-smi\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0c92a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline, AutoModelForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9729037",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'aubmindlab/bert-base-arabertv2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638a6d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691d4643",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline('ner', model=model_name, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2576cc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a741f07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = os.listdir('../../data/facebook_posts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef9fdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d5410a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "for filename in data:\n",
    "    df = pd.read_csv(f\"../../data/facebook_posts/{filename}\")\n",
    "    df_list.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf4212d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4eb78ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(df_list, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae1d02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fdf5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_message = df[[\"Facebook Id\",\"Total Interactions\", \"Message\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a33714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1114 posts with no message\n",
    "df_message.dropna(subset= [\"Message\"], inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d79f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual assignation of untrained model. Not needed furthermore.\n",
    "# df_message = df_message.assign(NER = lambda x: x[\"Message\"].apply(lambda s: classifier(s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc82865",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b75fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_message_small = df_message[:25].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a67bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_message_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5d16c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_dict_series(\n",
    "    df: pd.DataFrame,\n",
    "    dict_column: str,\n",
    "    keys = None,\n",
    "    col_prefix = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Expand a series of dicts into one column for each of the specified keys.\n",
    "\n",
    "    Note that an error will only be raised if keys is not None and all the keys\n",
    "    are not found in any dict in the series. If a key is missing from a\n",
    "    given row but is present in other rows, that row will be nan in\n",
    "    the resulting column.\n",
    "\n",
    "    Specify col_prefix in cases where keys in dict col overlap\n",
    "    with column names.\n",
    "\n",
    "    ref:\n",
    "    https://stackoverflow.com/questions/54344114/expand-pandas-dataframe-column-of-dict-into-dataframe-columns\n",
    "    \"\"\"\n",
    "    assert not df[dict_column].isnull().any(), \"na found in dict column\"\n",
    "    df = df.reset_index(drop=True)  # get a clean index for the join\n",
    "    expanded_df = pd.DataFrame(df[dict_column].values.tolist())\n",
    "    if col_prefix is not None:\n",
    "        expanded_df = expanded_df.add_prefix(col_prefix)\n",
    "    keys = keys or expanded_df.columns\n",
    "    expanded_df = df.drop(dict_column, axis=1).join(expanded_df[keys])\n",
    "    return expanded_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b06fe98",
   "metadata": {},
   "outputs": [],
   "source": [
    "NER_keys_df = expand_dict_series(df_message_small.explode(\"NER\"), \"NER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f199bbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "NER_keys_df.iloc[0].Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0536c628",
   "metadata": {},
   "outputs": [],
   "source": [
    "NER_keys_df[\"score\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d221ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "NER_keys_df.groupby(\"entity\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d96b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_entity_df = NER_keys_df.groupby([\"word\", \"entity\"]).count().sort_values(by=\"score\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c8b445",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_entity_df[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d5c350",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_entity_df.assign(word_en =  lambda x: x[\"word\"].apply(lambda s: translator.translate(s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcb2bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.data_path import DATA_DIRECTORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450fa15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list =['B-LOC',\n",
    " 'O',\n",
    " 'B-PERS',\n",
    " 'I-PERS',\n",
    " 'B-ORG',\n",
    " 'I-LOC',\n",
    " 'I-ORG',\n",
    " 'B-MISC',\n",
    " 'I-MISC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b244b097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROBABLY NOT NEEDED\n",
    "class NERDataset:\n",
    "    def __init__(self, texts, tags, label_list, model_name, max_length):\n",
    "        self.texts = texts\n",
    "        self.tags = tags\n",
    "        self.label_map = {label: i for i, label in enumerate(label_list)}\n",
    "        self.preprocessor = ArabertPreprocessor(model_name.split(\"/\")[-1])    \n",
    "        self.pad_token_label_id = torch.nn.CrossEntropyLoss().ignore_index\n",
    "        # Use cross entropy ignore_index as padding label id so that only\n",
    "        # real label ids contribute to the loss later.\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.max_length = max_length\n",
    "\n",
    "     \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "  \n",
    "    def __getitem__(self, item):\n",
    "        textlist = self.texts[item]\n",
    "#         tags = self.tags[item]\n",
    "\n",
    "        tokens = []\n",
    "        label_ids = []\n",
    "#         for word, label in zip(textlist, tags):      \n",
    "        for word in textlist:      \n",
    "            clean_word = self.preprocessor.preprocess(word)  \n",
    "            word_tokens = self.tokenizer.tokenize(clean_word)\n",
    "\n",
    "            if len(word_tokens) > 0:\n",
    "                tokens.extend(word_tokens)    \n",
    "                # Use the real label id for the first token of the word, and padding ids for the remaining tokens\n",
    "#                 label_ids.extend([self.label_map[label]] + [self.pad_token_label_id] * (len(word_tokens) - 1))\n",
    "                label_ids.extend([self.pad_token_label_id] + [self.pad_token_label_id] * (len(word_tokens) - 1))\n",
    " \n",
    "        # Account for [CLS] and [SEP] with \"- 2\" and with \"- 3\" for RoBERTa.\n",
    "        special_tokens_count = self.tokenizer.num_special_tokens_to_add()\n",
    "        if len(tokens) > self.max_length - special_tokens_count:\n",
    "            tokens = tokens[: (self.max_length - special_tokens_count)]\n",
    "            label_ids = label_ids[: (self.max_length - special_tokens_count)]\n",
    "\n",
    "        #Add the [SEP] token\n",
    "        tokens += [self.tokenizer.sep_token]\n",
    "        label_ids += [self.pad_token_label_id]\n",
    "        token_type_ids = [0] * len(tokens)\n",
    "\n",
    "        #Add the [CLS] TOKEN\n",
    "        tokens = [self.tokenizer.cls_token] + tokens\n",
    "        label_ids = [self.pad_token_label_id] + label_ids\n",
    "        token_type_ids = [0] + token_type_ids\n",
    "\n",
    "        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding_length = self.max_length - len(input_ids)\n",
    "\n",
    "        input_ids += [self.tokenizer.pad_token_id] * padding_length\n",
    "        attention_mask += [0] * padding_length\n",
    "        token_type_ids += [0] * padding_length\n",
    "        label_ids += [self.pad_token_label_id] * padding_length\n",
    "\n",
    "        assert len(input_ids) == self.max_length\n",
    "        assert len(attention_mask) == self.max_length\n",
    "        assert len(token_type_ids) == self.max_length\n",
    "        assert len(label_ids) == self.max_length\n",
    "\n",
    "        # if item < 5:\n",
    "        #   print(\"*** Example ***\")\n",
    "        #   print(\"tokens:\", \" \".join([str(x) for x in tokens]))\n",
    "        #   print(\"input_ids:\", \" \".join([str(x) for x in input_ids]))\n",
    "        #   print(\"attention_mask:\", \" \".join([str(x) for x in attention_mask]))\n",
    "        #   print(\"token_type_ids:\", \" \".join([str(x) for x in token_type_ids]))\n",
    "        #   print(\"label_ids:\", \" \".join([str(x) for x in label_ids]))\n",
    "\n",
    "        return {\n",
    "            'input_ids' : torch.tensor(input_ids, dtype=torch.long),\n",
    "            'attention_mask' : torch.tensor(attention_mask, dtype=torch.long),\n",
    "            'token_type_ids' : torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'labels' : torch.tensor(label_ids, dtype=torch.long)       \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f4c406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = DATA_DIRECTORY +\"/retrained_ner_arabertv2\"\n",
    "model_name = 'aubmindlab/bert-base-arabertv2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1526501",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyarabic\n",
    "# !git clone https://github.com/aub-mind/arabert\n",
    "!pip install farasapy\n",
    "from arabert.preprocess import ArabertPreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0c8b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## retrained: \n",
    "model_rt = AutoModelForTokenClassification.from_pretrained(DATA_DIRECTORY +\"/retrained_ner_arabertv2\")\n",
    "# tokenizer_rt = AutoTokenizer.from_pretrained(DATA_DIRECTORY +\"/retrained_ner_arabertv2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4479ed6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_rt = pipeline('token-classification', model=model_rt, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50843922",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rt = df_message[:100].assign(NER_rt =  lambda x: x[\"Message\"].apply(lambda s: classifier_rt(s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d9df9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list =['B-LOC',\n",
    " 'O',\n",
    " 'B-PERS',\n",
    " 'I-PERS',\n",
    " 'B-ORG',\n",
    " 'I-LOC',\n",
    " 'I-ORG',\n",
    " 'B-MISC',\n",
    " 'I-MISC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ab3cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label2ent= {\n",
    " \"LABEL_0\":'B-LOC',\n",
    " \"LABEL_1\": 'O',\n",
    " \"LABEL_2\": 'B-PERS',\n",
    " \"LABEL_3\": 'I-PERS',\n",
    " \"LABEL_4\": 'B-ORG',\n",
    " \"LABEL_5\": 'I-LOC',\n",
    " \"LABEL_6\": 'I-ORG',\n",
    " \"LABEL_7\": 'B-MISC',\n",
    " \"LABEL_8\": 'I-MISC'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c573d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indexed = df_rt.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c27d4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indexed= df_indexed.rename(columns={\"index\": \"post_id\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071057c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "NER_keys_df_rt = expand_dict_series(df_indexed.explode(\"NER_rt\"), \"NER_rt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dceb250",
   "metadata": {},
   "outputs": [],
   "source": [
    "NER_keys_df_rt[\"entity\"] = NER_keys_df_rt[\"entity\"].replace(label2ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcb5024",
   "metadata": {},
   "outputs": [],
   "source": [
    "NER_keys_df_rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f591ba08",
   "metadata": {},
   "outputs": [],
   "source": [
    "### concat entities together\n",
    "NER_keys_df_rt_entities = NER_keys_df_rt[NER_keys_df_rt[\"entity\"]!=\"O\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1711eb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "NER_keys_df_rt_entities= NER_keys_df_rt_entities.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe6940b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NER_keys_df_rt_entities.loc[i,\"entity\"][-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74547c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "NER_keys_df_rt_entities[\"word_plus_1\"] = NER_keys_df_rt_entities[\"word\"].shift(-1)\n",
    "NER_keys_df_rt_entities[\"ent_plus_1\"] = NER_keys_df_rt_entities[\"entity\"].shift(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccef24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NER_keys_df_rt_entities[\"word_new\"] = (NER_keys_df_rt_entities[\"word\"] + NER_keys_df_rt_entities[\"word_plus_1\"])if "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918d8651",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_condition(i:int, df) -> bool:\n",
    "    condition = ((df.loc[i,\"entity\"].startswith(\"I\") or df.loc[i,\"entity\"].startswith(\"B\")) and \n",
    "                 df.loc[i+1,\"entity\"].startswith(\"I\") and\n",
    "                 df.loc[i+1,\"entity\"][-3:] == df.loc[i,\"entity\"][-3:] and\n",
    "                 df.loc[i+1, \"start\"] == df.loc[i, \"end\"])\n",
    "    return condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f30b0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = NER_keys_df_rt_entities.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab225ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Concat words if they're B and I's of the same entity type\n",
    "for i in reversed(range(1, (len(df_2)-1))):\n",
    "    df_2.loc[i, \"word\"] = df_2.loc[i, \"word\"] +df_2.loc[i+1, \"word\"] if (concat_condition(i,df_2)) else df_2.loc[i, \"word\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b877cea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615152df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post = df_2.groupby(\"post_id\").first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e52395d",
   "metadata": {},
   "outputs": [],
   "source": [
    "NER_keys_df_rt_entities_1 = df_2.drop(df_2[df_2[\"entity\"].str.startswith(\"I\")].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea3cd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "NER_keys_df_rt_entities_1[\"word\"] = NER_keys_df_rt_entities_1[\"word\"].str.replace(\"#\", \"\", regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6929f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "NER_keys_df_rt_entities_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3871a5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = NER_keys_df_rt_entities_1[NER_keys_df_rt_entities_1[\"entity\"]!=\"O\"].groupby([\"post_id\", \"entity\"])[\"word\"].apply(list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72208827",
   "metadata": {},
   "outputs": [],
   "source": [
    "NER_keys_df_rt.iloc[0][\"Message\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e38fc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71ffbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_df = entities.unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8634548b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_df = ent_df.fillna(\"[]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325aabfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ec3fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post = df_post[[\"Facebook Id\",\"Total Interactions\", \"Message\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad468915",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post.merge(ent_df, left_index=True, right_index=True, how=\"left\").to_csv(\"Facebook_May.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f536835",
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_df[\"word\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a7e0e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
